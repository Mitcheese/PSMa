optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.000001,
  weight_decay : 0.1
}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 50,
    initial_epochs : 10
}}

dataset : {
  train : { _base_: configs/dataset_configs/FinetuneTrain.yaml,
            others: {subset: 'train', kpoints: 64}},
  val : { _base_: configs/dataset_configs/FinetuneTest.yaml,
            others: {subset: 'test', kpoints: 64}},
  test : { _base_: configs/dataset_configs/FinetuneTest.yaml,
            others: {subset: 'test', kpoints: 64}}}

model : {
  NAME: PointTransformer,
  trans_dim: 768,
  depth: 24,
  drop_path_rate: 0.1,
  cls_dim: 105,
  num_heads: 6,
  group_size: 64,
  num_group: 128,
  encoder_dims: 768,
}

npoints: 4096
total_bs : 32
step_per_update : 1
max_epoch : 80
grad_norm_clip : 10
